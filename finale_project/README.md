# Группа: DST-48

**Sergey Pinaev - binom1982@gmail.com**

# Финальный проект

Предсказание стоимости домов, основываясь на истории предложений.

Ссылка на соревнование: [[SF-DST] Car Price prediction](https://www.kaggle.com/c/sf-dst-car-price-prediction-part2)

<p align="center" width="100%">
<img src="https://habrastorage.org/r/w1560/getpro/habr/upload_files/080/2a6/5e7/0802a65e78ee2bd84388c0d1ebab98d5.png" width="auto"/>
</p>
<hr>

## Задача

Ко мне обратился представитель крупного агентства недвижимости со следующей проблемой:

Мои риелторы тратят катастрофически много времени на сортировку объявлений и поиск выгодных предложений. Поэтому их скорость реакции, да и, сказать по правде, качество анализа не дотягивает до уровня конкурентов. А это сказывается на наших финансовых показателях. Твоя задача — разработать модель, которая бы позволила обойти конкурентов по скорости и качеству совершения сделок. Датасет прикладываю.

**Цель**: разработать сервис, который будет предсказывать стоимость домов, основываясь на истории предложений.

## Описание датасета

В датасете 18 колонок

Целевая переменная **target** - цена объекта недвижимости

* **status** - статус объявления
* **private pool** - наличие бассейна, вероятно дублируется столбцом **PrivatePool**
* **propertyType** - тип объекта недвижимости
* **street**- адрес
* **baths** - количество ванных комнат
* **homeFacts**- информация о доме. Этот столбец надо парсить
* **fireplace** - наличие камина
* **city** - город
* **schools** - рейтинг и близость образовательных учреждений. Нужно парсить
* **sqft** - площадь в квадратных футах
* **zipcode** - почтовый индекс
* **beds**- количество спальен (или количество кроватей)
* **state** - штат
* **stories** - вероятно, количество владельцев или продаж этого дома
* **mls-id** - идентификационный номер в системе MLS. Вероятно, дополняется столбцом **MlsId**

## Этапы работы над проектом

1. Первичная обработка данных; 
2. EDA (анализ распределения признаков и замена пропусков);
3. Features Engineering;
4. Корреляционный анализ;
5. Обработка категориальных признаков, создание dummy переменных, и кодирование с помощью **Label encoding**;
6. Рекурсивное устранение признаков (RFE) и устранение тех, которые не влияют на предсказываемую величину;
7. Масштабирование признаков с помощью **MinMaxScaling**, **StandardScaler**
8. Выбор метрик для оценки качества: **MAE** и **MAPE**;
9. Логарифмирование целевой переменной;
10. Анализ данных на различных алгоритмах **Регрессии** : **LinearRegression**, **CatBoost**, **XGBoost** (с подбором гипермараметров и кросс-валидацией);
11. Применения **StackingRegressor** (решающая модель **CatBoostRegressor**) с **BaggingRegressor** (модели **GradientBoostingRegressor** и **XGBRegressor**);
12. Создание собственной нейронной модели;
13. Тестирование модели **ElasticNet**, **TabNet**;
14. Применение **Blend** (усреднение результата по двум лучшим моделям) для **StackingRegressor** и **TabNet**.

## Выводы

В результате **EDA** и **Feature engineering**, было создано достаточно много новых признаков, некоторые из которых имеют очень высокую корреляцию Пирсона. Тем не менее удаление сильно коррелируемых признаков в ручную или с помощью алгоритма RFE немного но ухудшало метрику. Поэтому данный шаг можно отсавить на будущее, когда уже модель пойдет в продакш и будет важна ее скорость.

Основной метрикой **MAE** и **MAPE**, так как первая показывает средний модуль отклонения, а вторая средний процент отклонения от цены.

Было опробовано много моделей машинного обучения, с подбором гипер параметров и применением крос-валидации и **Stacking**, а также некоторые нейронные сети.

Среди всех опробованных алгоритмов наилучший результат был получен при логарифмировании целевой переменной у моделей:

**Basemodel**

> Базовая модель: Линейная регрессия
> 
> MAE: 415583.13$
> 
> MAPE: 2.21%
> 
> R2: 0.08%

**XGBoost (лучшая модель)**
 
> MAE: 292464.92$
> 
> MAPE: 1.14%
> 
> R2: 0.36%

**Stacking**

> Решающая модель **CatBoostRegressor**
> 
> **BaggingRegressor** : модели **GradientBoostingRegressor** и **XGBRegressor**
> 
> Время - 79 минут
> 
> MAE: 298187.69$
> 
> MAPE: 1.28%
> 
> R2: 0.33%

**TabNet**

> 3000 - эпох, 5 - фолдов, время - 31 час на CPU (на GPU не заработала)
> 
> MAE: 321656.14$
> 
> MAPE: 1.07%
> 
> R2: 0.31%

**Blend**

> Усредненная оценка по двум лучшим моделям **Stacking** и **TabNet**
> 
> MAE: 300737.5$
> 
> MAPE: 1.09%
> 
> R2: 0.33%

Цели дипломного проекта выполнены.

Я использовал все навыки полученные на курсе и попробовал применить их в дипломном проекте.
